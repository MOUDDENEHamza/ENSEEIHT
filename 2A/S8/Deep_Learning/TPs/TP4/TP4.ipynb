{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMMppWbnG3dN"
   },
   "source": [
    "\n",
    "# Estimation de posture dans une image\n",
    "\n",
    "Pour ce TP ainsi que le suivant, nous allons traiter le problème de la détection du \"squelette\" d'un humain dans une image, tel qu'illustré dans la figure ci-dessous.\n",
    "\n",
    "![Texte alternatif…](https://drive.google.com/uc?id=1HpyLwzwkFdyQ6APoGZQJL7f837JCHNkh)\n",
    "\n",
    "Nous allons pour ce faire utiliser le [Leeds Sport Pose Dataset](https://sam.johnson.io/research/lspet.html) qui introduit 10000 images présentant des sportifs dans diverses situations, augmentées d'une annotation manuelle du squelette.\n",
    "\n",
    "À chaque image est associée une matrice de taille 3x14, correspondant aux coordonnées dans l'image des 14 joints du squelette de la personne décrite dans l'image. La 3e dimension désigne la visibilité du joint (1 s'il est visible, 0 s'il est occulté)\n",
    "\n",
    "Ces joints sont, dans l'ordre :\n",
    "*   Cheville droite\n",
    "*   Genou droit\n",
    "*   Hanche droite\n",
    "*   Hanche gauche\n",
    "*   Genou gauche\n",
    "*   Cheville gauche\n",
    "*   Poignet droit\n",
    "*   Coude droit\n",
    "*   Épaule droite\n",
    "*   Épaule gauche\n",
    "*   Coude gauche\n",
    "*   Poignet gauche\n",
    "*   Cou\n",
    "*   Sommet du crâne\n",
    "\n",
    "Pour un rappel des notions vues en cours sur ce sujet, vous pouvez regarder la vidéo ci-dessous :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"https://video.polymny.studio/?v=84ace9c1-f460-4375-9b33-917c3ff82c83/\", width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Méthodologie \n",
    "\n",
    "Pour résoudre ce problème, nous allons suivre une méthodologie similaire à celle présentée dans le 2e cours, et rappelée sur la figure suivante : \n",
    "\n",
    "![Méthodologie de développement d'un algorithme d'apprentissage profond](https://drive.google.com/uc?id=195pkcjca4r_g86KDt2LCe0QdQsMC6iba)\n",
    "\n",
    "Ainsi nous allons commencer par une modélisation simple du problème, construire un modèle et l'améliorer pas à pas et évaluer sa performance.\n",
    "Dans un second temps, nous modifierons la modélisation du problème, et donc l'architecture utilisée, afin d'améliorer les résultats.\n",
    "\n",
    "Pour chacune de ces deux étapes, je vous suggère de suivre la démarche suivante : \n",
    "\n",
    "- Simplifier le problème en traitant 10 imagettes (par exemple de dimension $64 \\times 64$) et construire un réseau qui surapprend parfaitement (qui diminue la perte jusqu'à quasiment 0)\n",
    "- Ajouter des images (~1000) et recalibrer le réseau pour à nouveau, obtenir un sur-apprentissage\n",
    "- Commencer à corriger le sur-apprentissage en ajoutant de la régularisation\n",
    "- Et enfin, utiliser l'ensemble de la base de données pour diminuer le sur-apprentissage au maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjqZNAX2CVi1"
   },
   "source": [
    "# Régression de la position des joints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFXG64qtCaCb"
   },
   "source": [
    "Dans un premier temps, et comme vu en cours, nous allons nous inspirer de l'algorithme DeepPose (**[Toshev et al.] DeepPose : Human Pose Estimation via Deep Neural Networks**) et formuler le problème comme une régression de la position (x,y) des joints dans l'espace de l'image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3mdNJJXc6Wy"
   },
   "source": [
    "Commencez par télécharger la base de données sur Github\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IVjmLKWRDag"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/axelcarlier/lsp.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-EFIogzdCc9"
   },
   "source": [
    "Le bloc suivant contient une fonction qui permet de charger les images de la base de données dans les variables x et y. Par défaut les images sont redimensionnées en taille 128$\\times$128 et la base de données contient 1000 images. Pour commencer et vous permettre de travailler plus efficacement, **je vous suggère très fortement de diminuer la dimension des images** (par exemple 64$\\times$64) **et de ne travailler que sur un ensemble réduit d'images** (par exemple, 10). \n",
    "\n",
    "\n",
    "N'oubliez pas également de diviser les données en images de test et/ou de validation pour obtenir des informations sur le sur-apprentissage éventuel. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quOHEF__pf36"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import os, sys\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Cette fonction permettra plus tard de charger plus ou moins d'images (en modifiant le paramètre num_images)\n",
    "# et de modifier la dimension d'entrée\n",
    "def load_data(image_size=128, num_images=1000):\n",
    "\n",
    "  path = \"./lsp/images/\"\n",
    "  dirs = sorted(os.listdir(path))\n",
    "\n",
    "  x = np.zeros((min(num_images,len(dirs)),image_size,image_size,3))\n",
    "  y = np.zeros((min(num_images,len(dirs)), 3, 14))\n",
    "    \n",
    "  #Chargement des joints    \n",
    "  mat_contents = loadmat('./lsp/joints.mat')\n",
    "  joints = mat_contents['joints']\n",
    "\n",
    "  # Chargement des images, qui sont rangées dans lsp/images\n",
    "  for i in range(min(num_images,len(dirs))):\n",
    "    item = dirs[i]\n",
    "    if os.path.isfile(path+item):\n",
    "        img = Image.open(path+item)\n",
    "        # Redimensionnement et sauvegarde des joints\n",
    "        y[i, 0] = joints[:,0,i]*image_size/img.size[0]\n",
    "        y[i, 1] = joints[:,1,i]*image_size/img.size[1]\n",
    "        y[i, 2] = joints[:,2,i]\n",
    "        # Redimensionnement et sauvegarde des images        \n",
    "        img = img.resize((image_size,image_size))\n",
    "        x[i] = np.asarray(img)\n",
    "\n",
    "\n",
    "  return x, y\n",
    "\n",
    "# Chargement de seulement 10 images, de taille 64x64\n",
    "x, y = load_data(image_size=64, num_images=10)           \n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRc0B4oxe6h_"
   },
   "outputs": [],
   "source": [
    "labels= {0: 'Cheville droite',\n",
    "         1: 'Genou droit',\n",
    "         2: 'Hanche droite',\n",
    "         3: 'Hanche gauche',\n",
    "         4: 'Genou gauche',\n",
    "         5: 'Cheville gauche',\n",
    "         6: 'Poignet droit',\n",
    "         7: 'Coude droit',\n",
    "         8: 'Épaule droite',\n",
    "         9: 'Épaule gauche',\n",
    "         10: 'Coude gauche',\n",
    "         11: 'Poignet gauche',\n",
    "         12: 'Cou',\n",
    "         13: 'Sommet du crâne'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meezS1y4G8QO"
   },
   "source": [
    "La fonction suivante vous permet de visualiser les données. Vous vous rendrez compte que certaines données sont manquantes ! En effet quand des joints sont occultés dans les images, des valeurs de position aberrantes (négatives) sont indiquées. Dans ce cas, nous n'afficherons pas les articulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvcqdQIZdCYk"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fonction d'affichage d'une image et de son label associé\n",
    "def print_data(x,y,i):\n",
    "    \n",
    "  if y.shape[1] < 3:\n",
    "    y_new = np.ones((y.shape[0], 3, y.shape[2]))\n",
    "    y_new[:,0:2,:] = y\n",
    "    y = y_new\n",
    "    \n",
    "  plt.figure(figsize=(5, 5))\n",
    "  plt.imshow(x[i]/255)\n",
    "  for j in range(0,14):\n",
    "    if y[i, 2, j] == 1:\n",
    "        plt.scatter(y[i,0,j],y[i,1,j],label=labels.get(j))\n",
    "\n",
    "  # Jambe droite      \n",
    "  if (y[i, 2, 0] + y[i, 2, 1] == 2):\n",
    "      plt.plot(y[i,0,0:2],y[i,1,0:2],'b')\n",
    "  # Cuisse droite      \n",
    "  if (y[i, 2, 1] + y[i, 2, 2] == 2):\n",
    "      plt.plot(y[i,0,1:3],y[i,1,1:3],'b')\n",
    "  # Bassin     \n",
    "  if (y[i, 2, 2] + y[i, 2, 3] == 2):\n",
    "      plt.plot(y[i,0,2:4],y[i,1,2:4],'b')\n",
    "  # Cuisse gauche      \n",
    "  if (y[i, 2, 3] + y[i, 2, 4] == 2):\n",
    "      plt.plot(y[i,0,3:5],y[i,1,3:5],'b')\n",
    "  # Jambe gauche      \n",
    "  if (y[i, 2, 4] + y[i, 2, 5] == 2):\n",
    "      plt.plot(y[i,0,4:6],y[i,1,4:6],'b')\n",
    "  # Avant-bras droit      \n",
    "  if (y[i, 2, 6] + y[i, 2, 7] == 2):\n",
    "      plt.plot(y[i,0,6:8],y[i,1,6:8],'b')\n",
    "  # Bras droit      \n",
    "  if (y[i, 2, 7] + y[i, 2, 8] == 2):\n",
    "      plt.plot(y[i,0,7:9],y[i,1,7:9],'b')\n",
    "  # Bras gauche     \n",
    "  if (y[i, 2, 9] + y[i, 2, 10] == 2):\n",
    "      plt.plot(y[i,0,9:11],y[i,1,9:11],'b')\n",
    "  # Avant-bras gauche      \n",
    "  if (y[i, 2, 10] + y[i, 2, 11] == 2):\n",
    "      plt.plot(y[i,0,10:12],y[i,1,10:12],'b') \n",
    "  # Buste droit\n",
    "  x1=[y[i,0,2],y[i,0,12]]\n",
    "  y1=[y[i,1,2],y[i,1,12]]\n",
    "  if (y[i, 2, 2] + y[i, 2, 12] == 2):\n",
    "      plt.plot(x1, y1,'b')\n",
    "  # Buste gauche\n",
    "  x1=[y[i,0,3],y[i,0,12]]\n",
    "  y1=[y[i,1,3],y[i,1,12]]\n",
    "  if (y[i, 2, 3] + y[i, 2, 12] == 2):\n",
    "      plt.plot(x1, y1,'b')\n",
    "  # Omoplate droite\n",
    "  x1=[y[i,0,8],y[i,0,12]]\n",
    "  y1=[y[i,1,8],y[i,1,12]]\n",
    "  if (y[i, 2, 8] + y[i, 2, 12] == 2):\n",
    "      plt.plot(x1, y1,'b')\n",
    "  # Omoplate gauche\n",
    "  x1=[y[i,0,9],y[i,0,12]]\n",
    "  y1=[y[i,1,9],y[i,1,12]]\n",
    "  if (y[i, 2, 9] + y[i, 2, 12] == 2):\n",
    "      plt.plot(x1, y1,'b')\n",
    "  # Tete     \n",
    "  if (y[i, 2, 12] + y[i, 2, 13] == 2):\n",
    "      plt.plot(y[i,0,12:14],y[i,1,12:14],'b')\n",
    "\n",
    "  plt.axis([0, x.shape[1], x.shape[2], 0])\n",
    "  plt.show()\n",
    "  #plt.legend()\n",
    "\n",
    "# Affichage aléatoire d'une image\n",
    "print_data(x,y,np.random.randint(x.shape[0]-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nous formulons ce problème comme une régression, nous allons utiliser pour évaluer nos réseaux de neurones l'erreur quadratique moyenne (fonction *MSE*). Cette fonction sera parfaite comme fonction de perte, mais elle ne permet pas d'appréhender les résultats de manière satisfaisante.\n",
    "\n",
    "Une métrique commune en estimation de posture est le **PCK0.5**, pour *Percentage of Correct Keypoints*. *0.5* correspond à un seuil en-deça duquel on considère qu'un joint est correctement estimé. Cette question du seuil est particulièrement sensible car il faut utiliser une valeur qui soit valable pour n'importe quelle image. La personne considérée peut apparaître plus ou moins largement sur l'image, de face ou de profil, ce qui fait qu'une erreur de prédiction sur un joint peut avoir une importance très grande ou très faible selon les cas.\n",
    "\n",
    "Pour résoudre cette ambiguïté, on considère dans la métrique du **PCK0.5** que la référence est la taille de la tête, définie par la distance entre le joint du cou et le joint de la tête sur la vérité terrain. Un joint prédit par le réseau sera considéré correct s'il est situé à une distance inférieure à la moitié (*0.5*) de la taille de la tête par rapport au joint réel. ([Andriluka et al.] 2D Human Pose Estimation: New Benchmark and State of the Art Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.matlib \n",
    "\n",
    "# Calcul du \"Percentage of Correct Keypoint\" avec seuil alpha :\n",
    "# On compte corrects les joints pour lesquels la distance entre valeurs réelle et prédite \n",
    "# est inférieure à alpha fois la dimension de la tête (c'est un peu arbitraire...)\n",
    "# On ne comptera pas les joints invisibles.\n",
    "# y_true est de dimension Nx3x14 et y_pred Nx2x14 (le réseau ne prédit pas la visibilité)\n",
    "def compute_PCK_alpha(y_true, y_pred, alpha=0.5):\n",
    "    # Calcul des seuils ; la taille de la tête est la distance entre joints 12 et 13\n",
    "    head_sizes = np.sqrt(np.square(y_true[:,0,13]-y_true[:,0,12])+np.square(y_true[:,1,13]-y_true[:,1,12]))\n",
    "    thresholds = alpha*head_sizes\n",
    "    thresholds = np.matlib.repmat(np.expand_dims(thresholds, 1), 1, 14)\n",
    "\n",
    "    # Calcul des distances inter-joints\n",
    "    joints_distances = np.sqrt(np.square(y_true[:,0,:]-y_pred[:,0,:]) + np.square(y_true[:,1,:]-y_pred[:,1,:]))\n",
    "\n",
    "    # Visibilité des joints de la vérité terrain\n",
    "    visibility = y_true[:,2,:]\n",
    "    \n",
    "    total_joints = np.count_nonzero(visibility==1)\n",
    "    correctly_predicted_joints = np.count_nonzero(np.logical_and(joints_distances<thresholds, visibility == 1))\n",
    "    \n",
    "    return correctly_predicted_joints/total_joints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme dit précédemment, on va utiliser l'erreur quadratique moyenne (*MSE*) comme fonction de coût pour entraîner notre réseau de neurones, et on peut également comme pour le TP2 utiliser l'erreur absolue moyenne (*MAE*) pour obtenir une estimation plus fine des performances de notre réseau pendant l'entraînement (on obtient une erreur moyenne en pixels, ce qui est plus simple à interpréter).\n",
    "\n",
    "Il y a cependant une subtilité importante évoquée un peu plus haut : certains joints sont invisibles, et ont des coordonnées négatives (pour, il faut l'avouer, une raison un peu inexplicable). Il est important de ne pas affecter l'apprentissage en faisant prédire ces valeurs négatives, insensées, au réseau. \n",
    "\n",
    "On doit donc implanter nous-même notre propre fonction de coût, qui ne va pas prendre en compte les joints invisibles. Pour cela, il faut savoir que la vérité-terrain contient en fait 3 valeurs pour chaque joint : les 2 premières sont ses coordonnées sur l'image, la 3e représente la visibilité du joint (1 s'il est visible, 0 sinon).\n",
    "\n",
    "La fonction *custom_mse*, définie juste en-dessous, réalise cette opération. Prenez le temps de comprendre ce qu'il s'y passe. **Remarque importante** : Ce code fait appel à des fonctions particulières du Backend de Keras, dont vous trouverez les détails sur [cette page](https://keras.rstudio.com/articles/backend.html). Ces fonctions doivent traiter des tenseurs, de type *Tensor* (et pas des tableaux numpy), car elles seront appelées pendant l'entraînement sur des variables internes à Tensorflow. Les fonctions utilisables sont également limitées car il faut pouvoir dériver la fonction *custom_mse* pour la rétropropagation des gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "# y_true : vérité terrain de dimension B x 3 x 14\n",
    "# y_pred : une prédiction de dimension B x 2 x 14 (on ne prédit pas la visibilité)\n",
    "# B est le nombre d'images considérées (par exemple, pourra être la taille d'un mini-batch)\n",
    "def custom_mse(y_true, y_pred):\n",
    "    # Changement de dimension : Bx3x14 -> Bx14x3\n",
    "    y_true = K.permute_dimensions(y_true, (0, 2, 1))\n",
    "    # Changement de dimension : Bx14x3 -> (B*14)x3\n",
    "    y_true = K.reshape(y_true, shape=(-1, 3))\n",
    "  \n",
    "    # Changement de dimension : Bx2x14 -> Bx14x2\n",
    "    y_pred = K.permute_dimensions(y_pred, (0, 2, 1))\n",
    "    # Changement de dimension : Bx14x2 -> (B*14)x2\n",
    "    y_pred = K.reshape(y_pred, shape=(-1, 2))\n",
    "    \n",
    "    # Détermination de l'indices des joints visibles\n",
    "    visible = K.greater_equal(y_true[:, 2], 1)  \n",
    "    indices = K.arange(0, K.shape(y_true)[0])\n",
    "    indices_visible = indices[visible]\n",
    "    \n",
    "    # Sélection des vérité-terrains et prédictions des joints visibles\n",
    "    y_true_visible = K.gather(y_true[:,0:2], indices_visible)\n",
    "    y_pred_visible = K.gather(y_pred, indices_visible)\n",
    "    \n",
    "    # Calcul de la MSE\n",
    "    return K.mean(K.square(y_pred_visible[:,0] - y_true_visible[:,0]) + K.square(y_pred_visible[:,1] - y_true_visible[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous avez bien compris le code de *custom_mse*, vous devriez pouvoir sans trop de problèmes écrire le code pour la fonction *custom_mae* ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true : vérité terrain de dimension B x 3 x 14\n",
    "# y_pred : une prédiction de dimension B x 2 x 14 (on ne prédit pas la visibilité)\n",
    "# B est le nombre d'images considérées (par exemple, pourra être la taille d'un mini-batch)\n",
    "def custom_mae(y_true, y_pred):\n",
    "    # ... A COMPLETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme d'habitude, on peut monitorer l'entraînement grâce à la fonction suivante (adaptée à nos fonctions *custom_mse* et *custom_mae* définies juste avant) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_analysis(history):\n",
    "  mae = history.history['custom_mae']\n",
    "  val_mae = history.history['val_custom_mae']\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  epochs = range(len(loss))\n",
    "\n",
    "  plt.plot(epochs, mae, 'b', linestyle=\"--\",label='Training MAE')\n",
    "  plt.plot(epochs, val_mae, 'g', label='Validation MAE')\n",
    "  plt.title('Training and validation MAE')\n",
    "  plt.legend()\n",
    "\n",
    "  plt.figure()\n",
    "\n",
    "  plt.plot(epochs, loss, 'b', linestyle=\"--\",label='Training loss')\n",
    "  plt.plot(epochs, val_loss,'g', label='Validation loss')\n",
    "  plt.title('Training and validation loss')\n",
    "  plt.legend()\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A vous de jouer :\n",
    "\n",
    "Pour tenter de résoudre le problème, vous pouvez suivre les étapes suivantes : \n",
    "\n",
    "- Simplifier le problème en traitant 10 imagettes (par exemple de dimension $64 \\times 64$) et construire un réseau qui surapprend parfaitement (qui diminue la perte jusqu'à quasiment 0)\n",
    "- Ajouter des images (~1000) et éventuellement recalibrer votre réseau pour à nouveau, obtenir un sur-apprentissage\n",
    "- Commencer à corriger le sur-apprentissage en ajoutant de la régularisation (notamment sur les couches denses)\n",
    "- Et enfin, utiliser l'ensemble de la base de données pour diminuer le sur-apprentissage au maximum\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "IAM2020 - TP4 - Estimation de Posture.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
