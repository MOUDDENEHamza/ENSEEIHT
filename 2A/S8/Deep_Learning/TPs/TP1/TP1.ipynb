{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 1 - Prise en main de Keras\n",
    "\n",
    "Dans ce TP, vous allez pouvoir prendre en main Keras pour construire et entraîner vos premiers réseaux de neurones. Nous vous fournissons une base de code permettant de reproduire un exemple simple du type de ceux présentés sur le site [playground.tensorflow.org](http://playground.tensorflow.org). L'objectif du TP est de modifier ce code afin d'illustrer certains concepts vus en cours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAoaQM9VMmAD"
   },
   "source": [
    "# Génération des données\n",
    "\n",
    "Génération de données ressemblant aux données \"circulaires\" du playground : il s'agit d'un nuage de points en deux dimensions répartis en deux classes (\"bleu\" et \"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2621,
     "status": "ok",
     "timestamp": 1602056140214,
     "user": {
      "displayName": "Axel Carlier",
      "photoUrl": "",
      "userId": "04558954327840318068"
     },
     "user_tz": -120
    },
    "id": "o9kpOoxkGl0y"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Le paramètre noise désigne le pourcentage du nombre de données n que l'on veut bruiter\n",
    "def generateData(n, noise=0):\n",
    "\n",
    "  a = np.random.rand(n) * 2 * np.pi\n",
    "  c1 = (1.5+np.random.rand(n)*2)*np.random.rand(n)*np.vstack((np.cos(a), np.sin(a)))\n",
    "\n",
    "  b = np.random.rand(n) * 2 * np.pi\n",
    "  c2 = (3+np.random.rand(n)+np.random.rand(n)*2)*np.vstack((np.cos(b), np.sin(b)))\n",
    "\n",
    "  x = np.concatenate((c1,c2), axis=1)\n",
    "  x = np.transpose(x)  \n",
    "  y = np.concatenate((np.zeros((n)),np.ones((n))),axis=0)\n",
    "  \n",
    "  permutation = np.random.permutation(2*n)   \n",
    "  x = x[permutation]\n",
    "  y = y[permutation]\n",
    "    \n",
    "  y[0:math.floor(n*noise/100)] = 1 -  y[0:math.floor(n*noise/100)] \n",
    "\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxmeLW1HMe6M"
   },
   "source": [
    "Affichage des données générées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 2609,
     "status": "ok",
     "timestamp": 1602056140215,
     "user": {
      "displayName": "Axel Carlier",
      "photoUrl": "",
      "userId": "04558954327840318068"
     },
     "user_tz": -120
    },
    "id": "hsyb-vT-Gl01",
    "outputId": "24aaedd9-e5b8-457c-b405-3df4dd4a71e1"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x, y = generateData(200, noise=10)\n",
    "\n",
    "plt.plot(x[:,0][np.where(y == 0)], x[:,1][np.where(y == 0)], '+', label='+')\n",
    "plt.plot(x[:,0][np.where(y == 1)], x[:,1][np.where(y == 1)], 'o', label='o')\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPY8wM1Iouy-"
   },
   "source": [
    "Fonction utile par la suite d'affichage de l'évolution de la perte (erreur d'apprentissage et de validation) au cours de l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 700,
     "status": "ok",
     "timestamp": 1602056231336,
     "user": {
      "displayName": "Axel Carlier",
      "photoUrl": "",
      "userId": "04558954327840318068"
     },
     "user_tz": -120
    },
    "id": "Rp0LNjswouai"
   },
   "outputs": [],
   "source": [
    "def plot_loss(val_loss, train_loss):\n",
    "  plt.plot(val_loss, color='green', label='Erreur de validation')\n",
    "  plt.plot(train_loss, color='blue', linestyle='--', label='Erreur d\\'entraînement')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylim(0, 1)\n",
    "  plt.title('Évolution de la perte sur les ensembles d\\'apprentissage et de validation au cours de l\\'apprentissage')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBfeftq0Mp1h"
   },
   "source": [
    "Construction du modèle : ici le modèle a une seule couche cachée à 2 neurones avec une fonction d'activation relu, et une couche de sortie à 1 neurone avec une activation sigmoide (classification binaire !)\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1Aa9fmTa9hvm8sLCn30ZtTJrMd2m5TuBo\">\n",
    "<caption><center> Figure 1: réseau de neurones implémenté ci-dessous </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6278,
     "status": "ok",
     "timestamp": 1602056163954,
     "user": {
      "displayName": "Axel Carlier",
      "photoUrl": "",
      "userId": "04558954327840318068"
     },
     "user_tz": -120
    },
    "id": "77yyS_8IQC-T",
    "outputId": "8a64625a-613d-4bce-9126-b5f2284094a4"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2, activation='relu', input_dim=2)) # input_dim indique la dimension de la couche d'entrée, ici 2 (x1 et x2)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary() # affiche un résumé du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfrOE122Ms07"
   },
   "source": [
    "Mise en place de l'optimisation par descente de gradient stochastique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19640,
     "status": "ok",
     "timestamp": 1602056186643,
     "user": {
      "displayName": "Axel Carlier",
      "photoUrl": "",
      "userId": "04558954327840318068"
     },
     "user_tz": -120
    },
    "id": "Vrj-igwbRU98",
    "outputId": "7527eea8-e8f6-4199-837e-565e8356ef14"
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.03) # On choisit la descente de gradient stochastique, avec un taux d'apprentssage de 0.03\n",
    "\n",
    "# On définit ici, pour le modèle introduit plus tôt, l'optimiseur choisi, la fonction de perte (ici\n",
    "# l'entropie croisée car classification binaire) et les métriques que l'on veut observer pendant\n",
    "# l'entraînement. L'accuracy est le pourcentage de bonnes classifications.\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Entraînement du modèle avec des mini-batchs de taille 10, sur 200 epochs. \n",
    "# Le paramètre validation_split signifie qu'on tire aléatoirement une partie des données\n",
    "# (ici 20%) pour servir d'ensemble de validation\n",
    "history = model.fit(x, y, validation_split=0.2, epochs=200, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vO3wIAsOM0PO"
   },
   "source": [
    "Affichage de l'évolution de la perte puis évaluation sur un nouveau jeu de données, qui agit donc comme un ensemble de test (différent des ensembles d'apprentissage et de validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 794,
     "status": "ok",
     "timestamp": 1602056235438,
     "user": {
      "displayName": "Axel Carlier",
      "photoUrl": "",
      "userId": "04558954327840318068"
     },
     "user_tz": -120
    },
    "id": "zqPAndq0i2t7",
    "outputId": "b11b39ed-2194-4b4e-fa88-1c6cf59c05e7"
   },
   "outputs": [],
   "source": [
    "val_loss=(history.history['val_loss'])\n",
    "train_loss=(history.history['loss'])\n",
    "plot_loss(val_loss, train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 28758,
     "status": "ok",
     "timestamp": 1601996060192,
     "user": {
      "displayName": "Axel Carlier",
      "photoUrl": "",
      "userId": "04558954327840318068"
     },
     "user_tz": -120
    },
    "id": "9ytMjFw_I2Lo",
    "outputId": "62435072-e3cb-4095-dc87-007abc2ec40d"
   },
   "outputs": [],
   "source": [
    "x_test, y_test = generateData(100, noise=10)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlAKnroaM-jq"
   },
   "source": [
    "# Travail à effectuer\n",
    "\n",
    "Votre objectif pendant ce TP est de prendre en main Keras en illustrant quelques concepts vus en cours, et rappelés sur les vidéos ci-dessous.\n",
    "\n",
    "**1 - Évanescence des gradients avec la fonction sigmoïde et correction grâce à la fonction reLU.\n",
    "(la cellule ci-après affiche une vidéo qui illustre cet exemple).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"https://video.polymny.studio/?v=82ed9784-4f22-4922-ac9a-08c298e983f4/\", width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commencez par créer un réseau à 3 couches cachées de 4 neurones chacune, portant une fonction d'activation sigmoide, et essayez d'entraîner ce réseau. Vous devriez observer que la fonction de perte n'évolue pas au cours du temps, ceci à cause de l'evanescence des gradients. \n",
    "\n",
    "Dans un second temps, remplacez la fonction d'activation des couches cachées par une reLU et observez l'effet immédiat de ce changement sur l'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 - Ajout d'un terme de régularisation L1 ou L2 et impact sur le sur-apprentissage.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(\"https://video.polymny.studio/?v=e9580db4-8c91-486e-bb67-2edbdcfa887b/\", width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6jvSb1rj9og"
   },
   "source": [
    "Commencez d'abord par générer des données bruitées (paramètre *noise* à 50)\n",
    "\n",
    "Comme expliqué dans la vidéo, créez un réseau à 6 couches cachées et n'hésitez pas à utiliser un grand nombre de neurones dans ces couches cachées (par exemple, 50) afin d'obtenir un fort sur-apprentissage. \n",
    "Le paramètre *kernel_regularizer* des couches de Keras vous permettra d'ajouter une régularisation L1 et L2.\n",
    "\n",
    "Afin de bien voir l'impact de chacun des types de régularisation, utilisez la fonction *print_model_weights* ci-dessous pour afficher un histogramme des valeurs des paramètres de votre modèle à l'issue de l'apprentissage. Vous devriez observer, comme indiqué en cours, qu'une régularisation L2 fait tendre tous les paramètres vers 0, alors qu'une régularisation L1 produit un modèle creux avec beaucoup de valeurs nulles.\n",
    "\n",
    "```python\n",
    "def print_model_weights(model):\n",
    "    \n",
    "    model_parameters = []\n",
    "    for layer in model.layers:\n",
    "        weights = layer.get_weights() # liste de tableaux numpy\n",
    "        for w in weights:\n",
    "            model_parameters = np.concatenate((model_parameters, w), axis=None)\n",
    "\n",
    "    # Affichage d'un histogramme des paramètres du modèle, avec une échelle logarithmique symétrique\n",
    "    plt.hist(model_parameters, bins=[-10, -1, -0.1, -0.01, -0.001, 0.001, 0.01, 0.1, 1, 10])\n",
    "    plt.xscale('symlog', linthresh=0.01)\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "**3 - Impact du taux d'apprentissage** (facultatif, si vous en avez le temps !)\n",
    "\n",
    "Reprenez l'exemple de code qui vous est fourni initialement, en changeant simplement le nombre de neurones de la couche cachée (à modifier de 2 à 4). Jusqu'à présent, vous n'avez pas eu à remettre en cause le choix du taux d'apprentissage, positionné à 0.03.\n",
    "Dans ce dernier exercice, il vous est demandé de tester 5 valeurs de taux d'apprentissage : 3, 1, 0.1, 0.01, et 0.001 \n",
    "Exécutez l'apprentissage (à l'aide d'une descente de gradient stochastique) sur 100 epochs pour ces 5 valeurs et tracez une figure regroupant les courbes d'apprentissage correspondant aux 5 taux d'apprentissage.\n",
    "\n",
    "Comment interpréter les résultats ?\n",
    "\n",
    "\n",
    "Une fois ce code écrit, exécutez le à nouveau en remplaçant la descente de gradient stochastique (SGD) par l'algorithme Adam. Qu'observez-vous ?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IAM2020 - TP1 - Corrigé .ipynb",
   "provenance": [
    {
     "file_id": "1cienunElXY1OVX4CdUbh9KJ5Ou6Az9P2",
     "timestamp": 1601995868653
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
